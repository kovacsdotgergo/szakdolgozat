{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kovacsdotgergo/szakdolgozat/blob/feature%2Fcolab_bringup/esc_notebook.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/kovacsdotgergo/szakdolgozat.git\n",
    "%cd szakdolgozat\n",
    "!pip install wget torch torchvision torchaudio matplotlib pandas numpy timm==0.4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: tmp for branch\n",
    "!git branch\n",
    "!git checkout feature/colab_bringup\n",
    "!git branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "esc_path, save_path, workspace_path, have_cuda = utils.setup_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import ASTModel\n",
    "import torch\n",
    "## Model\n",
    "INPUT_TDIM = 512\n",
    "audio_model = ASTModel(label_dim=50, input_tdim=INPUT_TDIM, imagenet_pretrain=True, audioset_pretrain=True)\n",
    "audio_model = torch.nn.DataParallel(audio_model, device_ids=[0])\n",
    "audio_model = audio_model.to(torch.device(\"cuda:0\" if have_cuda else 'cpu'))\n",
    "target_len = INPUT_TDIM\n",
    "model_save_path = save_path + '/transformer.pth'\n",
    "train_epochs = 20\n",
    "train_proc_title = f'transformer {train_epochs} epoch training'\n",
    "lr = 5e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cnn\n",
    "import torch\n",
    "## Model\n",
    "audio_model = cnn.conv2d_v1()\n",
    "audio_model = torch.nn.DataParallel(audio_model, device_ids=[0])\n",
    "audio_model = audio_model.to(torch.device(\"cuda:0\" if have_cuda else 'cpu'))\n",
    "target_len = 512\n",
    "model_save_path = save_path + '/cnn2d_v1.pth'\n",
    "train_epochs = 80\n",
    "train_proc_title = f'CNN {train_epochs} epoch training'\n",
    "lr = 0.0009\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for saving the best model\n",
    "SAVE_PATH = '/kaggle/working/lstm_1lay_mel.pth'\n",
    "\n",
    "class LSTM_mel(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "    super().__init__()\n",
    "    self.num_layers = num_layers\n",
    "    self.hidden_size = hidden_size\n",
    "    self.lstm = nn.LSTM(input_size=input_size, hidden_size=self.hidden_size,\n",
    "                        num_layers=self.num_layers, batch_first=True)\n",
    "    self.fc = nn.Linear(self.hidden_size, output_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    x: (batch, mels, time_windows)\n",
    "    \"\"\"\n",
    "    h_0 = torch.autograd.Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "    c_0 = torch.autograd.Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "    if have_cuda:\n",
    "        h_0 = h_0.cuda()\n",
    "        c_0 = c_0.cuda()\n",
    "    #x = x.transpose(1, 2)\n",
    "    out, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "    h_n = h_n.view(-1, self.hidden_size)\n",
    "    return self.fc(h_n)\n",
    "\n",
    "INPUT_SIZE = 128\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 1\n",
    "OUTPUT_SIZE = 50\n",
    "\n",
    "audio_model = LSTM_mel(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, OUTPUT_SIZE)\n",
    "audio_model = torch.nn.DataParallel(audio_model, device_ids=[0])\n",
    "audio_model = audio_model.to(torch.device(\"cuda:0\" if have_cuda else 'cpu'))\n",
    "target_len = 512\n",
    "model_save_path = save_path + '/lstm.pth'\n",
    "train_epochs = 5\n",
    "train_proc_title = f'LSTM {train_epochs} epoch training'\n",
    "lr = 0.001\n",
    "\n",
    "## Dataset\n",
    "dataset = esc_dataset.ESCdataset(esc_path, n_fft=1024, hop_length=512,\n",
    "                     n_mels=128, augment=False,  log_mel=True,\n",
    "                     use_kaldi=True, target_len=None, resample_rate=22500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import esc_dataset\n",
    "import trainer\n",
    "import numpy as np\n",
    "\n",
    "## Dataset\n",
    "dataset = esc_dataset.ESCdataset(esc_path, n_fft=1024, hop_length=256,\n",
    "                     n_mels=128, augment=False,  log_mel=True,\n",
    "                     use_kaldi=True, target_len=target_len, resample_rate=22500)\n",
    "\n",
    "#dividing the dataset randomly, 80% train, 10% validation, 10% test\n",
    "numtrain = int(0.8*len(dataset))\n",
    "numval = (len(dataset) - numtrain) // 2\n",
    "numtest = len(dataset) - numtrain - numval\n",
    "split_dataset = torch.utils.data.random_split(dataset, [numtrain, numval, numtest])\n",
    "#using augment on the training data\n",
    "#split_dataset[0].augment = True\n",
    "\n",
    "## DataLoader\n",
    "BATCHSIZE = 16\n",
    "trainloader = torch.utils.data.DataLoader(split_dataset[0], batch_size=BATCHSIZE,\n",
    "                         shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(split_dataset[1], batch_size=BATCHSIZE, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(split_dataset[2], batch_size=BATCHSIZE, shuffle=True)\n",
    "\n",
    "## Trainer\n",
    "trainer = trainer.Trainer(audio_model, have_cuda, criterion=nn.CrossEntropyLoss)\n",
    "\n",
    "## Inference\n",
    "spect, label = dataset[0]\n",
    "print(f'trainer inference: {dataset.get_class_name(trainer.inference(spect, ret_index=True).item())}, '\n",
    "    f'true label: {dataset.get_class_name(label)}')\n",
    "\n",
    "## Training\n",
    "lrs = np.logspace(-1, -6, num=5)\n",
    "params = trainer.hyperparameter_plotting(lrs, trainloader, valloader, train_epochs=5)\n",
    "print(params)\n",
    "# trainer.train(trainloader, valloader, optimizer=torch.optim.AdamW, train_epochs=train_epochs,\n",
    "#               val_interval=25, lr=lr, save_best_model=True, save_path=model_save_path)\n",
    "# trainer.plot_train_proc(train_proc_title)\n",
    "\n",
    "# ## Test\n",
    "# trainer.load_model(model_save_path)\n",
    "# print(f'test accuracy: {trainer.test(testloader)}')\n",
    "\n",
    "# ## Inference after training\n",
    "# print(f'Trainer inference after training: {dataset.get_class_name(trainer.inference(spect, ret_index=True).item())}, '\n",
    "#     f'true label: {dataset.get_class_name(label)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c5440d9220ddd61252669e50fcd27d4d057d7cf15fbe79bfa9bf1a741db3cc6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
